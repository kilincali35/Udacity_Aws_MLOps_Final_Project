{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of the TODO's and/or use more than one cell to complete all the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Install any packages that you might need\n",
    "!pip install smdebug -q -U\n",
    "!pip install torchvision -q\n",
    "!pip install timm -q\n",
    "!pip install bokeh -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade sagemaker==2.219.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh was needed for ProfileReport, but new version was raising error, older version is working fine. \n",
    "!pip install --upgrade bokeh==2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import any packages that you might need\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.tuner import (IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner, HyperparameterTuningJobAnalytics)\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import (Rule, ProfilerRule, rule_configs, DebuggerHookConfig, ProfilerConfig, FrameworkProfile)\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import IdentitySerializer, JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.estimator import Estimator\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "import boto3\n",
    "import shutil\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_config = {\n",
    "    'run_hyperparameter_job': 0,  # 1 if you want to re-train hyperparameter job; 0 if you want to fetch already trained one\n",
    "    'train_a_model': 0  # 1 if you want to re-train a model, 0 if you want to fetch an already trained model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basic and default session opening method\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Initialize the SageMaker client\n",
    "# Session method over boto3 is more customizable, you can decide on several parameters like aws key etc. \n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "sagemaker_session = Session(boto_session=session)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default Bucket: {bucket}\")\n",
    "print(f\"RoleArn: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"s3://sagemaker-us-east-1-462848125024/aws_capstone_project/train_data/\"\n",
    "valid = \"s3://sagemaker-us-east-1-462848125024/aws_capstone_project/valid_data/\"\n",
    "test = \"s3://sagemaker-us-east-1-462848125024/aws_capstone_project/test_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we are defining some functions that will be used inside the notebook\n",
    "\n",
    "def train_hyperparameter_tuning_job(train, test, valid):\n",
    "    \"\"\"\n",
    "    This function is responsible for fitting a hyperparameter job from scratch if there is not any, or you want to train one from scratch. \n",
    "    Hyperparameter ranges and selections need to be configured inside this function, they are not embedded into a seperate config.\n",
    "    Also PyTorch and Tuner object settings need to be configured here. \n",
    "\n",
    "    Inputs:\n",
    "        train: You need to give train data directory from S3 object\n",
    "        test: You need to give test data directory from S3 object\n",
    "        valid: You need to give validation data directory from S3 object\n",
    "\n",
    "    Outputs:\n",
    "        best_estimator: This is the best estimator object from best hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    hyperparameter_ranges = {\n",
    "        \"batch-size\": CategoricalParameter([8, 128]),\n",
    "        \"epochs\": IntegerParameter(4, 16)\n",
    "    }\n",
    "\n",
    "    objective_metric_name = \"average test loss\"\n",
    "    objective_type = \"Minimize\"\n",
    "    metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test Loss: ([+-]?[0-9\\\\.]+)\"}]\n",
    "\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"hpo_revised.py\",\n",
    "        base_job_name='pyt-hpo-rev',\n",
    "        role=role,\n",
    "        py_version=\"py38\",\n",
    "        framework_version=\"1.12\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.g4dn.xlarge\"\n",
    "    )\n",
    "\n",
    "    tuner = HyperparameterTuner(\n",
    "        estimator=estimator,\n",
    "        early_stopping_type=\"Auto\",\n",
    "        metric_definitions=metric_definitions,\n",
    "        objective_metric_name=objective_metric_name,\n",
    "        objective_type=objective_type,\n",
    "        max_jobs=4,\n",
    "        max_parallel_jobs=2,\n",
    "        hyperparameter_ranges=hyperparameter_ranges,\n",
    "        base_tuning_job_name='pyt-hpo-rev'\n",
    "    )\n",
    "\n",
    "    tuner.fit(\n",
    "        {\n",
    "            \"training\": train,\n",
    "            \"valid\": valid,\n",
    "            \"test\": test,\n",
    "        },\n",
    "        wait=True\n",
    "    )\n",
    "\n",
    "    best_estimator = tuner.best_estimator()\n",
    "\n",
    "    best_estimator.hyperparameters()\n",
    "\n",
    "    return best_estimator, tuner\n",
    "\n",
    "def train_hyp_tuning_job_summarizer(tuner):\n",
    "    \"\"\"\n",
    "    This function summarizes the information related to trained job, prints this information and also shares the tuning_job_name\n",
    "\n",
    "    Inputs:\n",
    "        tuner: tuner object from training function\n",
    "\n",
    "    Outputs:\n",
    "        tuning_job_name: name of the tuning job from trained hpo job\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    tuning_job_name = tuner.latest_tuning_job.name\n",
    "    print(f\"Latest hyperparameter job name : {tuning_job_name}\")\n",
    "\n",
    "    # Initialize HyperparameterTuningJobAnalytics to get the best training job\n",
    "    tuner_analytics = HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "    # Get the best training job details\n",
    "    best_training_job = tuner_analytics.dataframe().sort_values('FinalObjectiveValue', ascending=True).iloc[0]\n",
    "\n",
    "    best_training_job_name = best_training_job['TrainingJobName']\n",
    "    print(f\"Best training job: {best_training_job_name}\")\n",
    "\n",
    "    # Describe the best training job to get its details, including hyperparameters\n",
    "    best_training_job_description = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "\n",
    "    # Access the hyperparameters\n",
    "    best_hyperparameters = best_training_job_description.get('HyperParameters', {})\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "\n",
    "    return tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hyperparameter_tuning_job(sagemaker_client):\n",
    "    \"\"\"\n",
    "    This function is responsible for fetching the latest hyperparameter job trained, its best hyperparameters and the best estimator object. \n",
    "    So that you will not need to re-traing the job from scratch and you will be able to continue with other cells. \n",
    "\n",
    "    Inputs:\n",
    "        sagemaker_client: client information from sagemaker\n",
    "    Outputs:\n",
    "        best_estimator: This is the best estimator object from best hyperparameters\n",
    "        latest_hyperparameter_tuning_job_name: name of the latest hyperparameter job trained\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of hyperparameter tuning jobs\n",
    "    tuning_jobs = sagemaker_client.list_hyper_parameter_tuning_jobs(\n",
    "        NameContains='pyt-hpo-rev',\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=10\n",
    "    )\n",
    "    print(tuning_jobs)\n",
    "    if not tuning_jobs['HyperParameterTuningJobSummaries']:\n",
    "        raise Exception(\"No hyperparameter tuning jobs found\")\n",
    "\n",
    "    latest_tuning_job = tuning_jobs['HyperParameterTuningJobSummaries'][0]\n",
    "    tuning_job_name = latest_tuning_job['HyperParameterTuningJobName']\n",
    "\n",
    "    print(f\"Latest tuning job: {tuning_job_name}\")\n",
    "\n",
    "    # Initialize HyperparameterTuningJobAnalytics to get the best training job\n",
    "    tuner_analytics = HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "    # Get the best training job details\n",
    "    best_training_job = tuner_analytics.dataframe().sort_values('FinalObjectiveValue', ascending=True).iloc[0]\n",
    "\n",
    "    best_training_job_name = best_training_job['TrainingJobName']\n",
    "    print(f\"Best training job: {best_training_job_name}\")\n",
    "\n",
    "    # Describe the best training job to get its details, including hyperparameters\n",
    "    best_training_job_description = sagemaker_client.describe_training_job(TrainingJobName=best_training_job_name)\n",
    "\n",
    "    # Access the hyperparameters\n",
    "    best_hyperparameters = best_training_job_description.get('HyperParameters', {})\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "\n",
    "    # Describe the best training job to get necessary details\n",
    "    best_training_job_description = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=best_training_job_name\n",
    "    )\n",
    "\n",
    "    model_artifact = best_training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "    # Create the estimator object using the best training job details\n",
    "    best_estimator = Estimator(\n",
    "        model_data=model_artifact,\n",
    "        role=role,\n",
    "        image_uri=best_training_job_description['AlgorithmSpecification']['TrainingImage'],\n",
    "        instance_count=best_training_job_description['ResourceConfig']['InstanceCount'],\n",
    "        instance_type=best_training_job_description['ResourceConfig']['InstanceType'],\n",
    "        hyperparameters=best_hyperparameters\n",
    "    )\n",
    "\n",
    "    print(f\"Best estimator created with model artifact: {model_artifact}\")\n",
    "\n",
    "    return best_estimator, tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_model_from_scratch(train, test, valid, role, best_hyperparameters, rules, profiler_config, debugger_config):\n",
    "    \"\"\"\n",
    "    This function aims to train a prediction model from scratch with the best hyperparameters found before. It also runs with\n",
    "    proper settings to apply a ProfilerReport at future stages.\n",
    "    You need to set the proper PyTorch configurations inside this function, they are not embedded inside config dictionary\n",
    "\n",
    "    Inputs:\n",
    "        train: You need to give train data directory from S3 object\n",
    "        test: You need to give test data directory from S3 object\n",
    "        valid: You need to give validation data directory from S3 object\n",
    "        role: get the role for sagemaker\n",
    "        best_hyperparameters: get the best hyperparameters dicitonary\n",
    "        rules: get rules dictionary\n",
    "        profiler_config: get profiler configurations\n",
    "        debugger_config: get debugger configurations\n",
    "\n",
    "    Outputs:\n",
    "        best_estimator: This is the best estimator object from best hyperparameters\n",
    "        latest_hyperparameter_tuning_job_name: name of the latest hyperparameter job trained with this function\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = PyTorch(\n",
    "        entry_point=\"train_revised.py\",\n",
    "        framework_version=\"1.12\",\n",
    "        base_job_name='pytorch-train-rev',\n",
    "        py_version=\"py38\",\n",
    "        role=role,\n",
    "        instance_count=2,\n",
    "        instance_type=\"ml.g4dn.xlarge\",\n",
    "        hyperparameters=best_hyperparameters,\n",
    "        rules=rules,\n",
    "        profiler_config=profiler_config,\n",
    "        debugger_hook_config=debugger_config,\n",
    "    )\n",
    "\n",
    "    estimator.fit(\n",
    "        {\n",
    "            \"training\": train, \n",
    "            \"valid\": valid,\n",
    "            \"test\": test,\n",
    "        }, \n",
    "        wait=True\n",
    "    )\n",
    "\n",
    "    training_job_name = estimator.latest_training_job.name\n",
    "    print(f\"Training jobname: {training_job_name}\")\n",
    "\n",
    "    return estimator, training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_a_trained_model():\n",
    "    \"\"\"\n",
    "        This function is used to get the latest trained job on the sagemaker directory. You need to set base_job_name parameter\n",
    "    inside this function. Also, other parameters if necessary\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "        estimator: returns the estimator object from the latest trained job\n",
    "        latest_training_job_name: returns the training job name from the latest trained job\n",
    "    \"\"\"\n",
    "    \n",
    "    base_job_name = 'pytorch-train-rev'\n",
    "\n",
    "    training_jobs = sagemaker_client.list_training_jobs(\n",
    "        NameContains=base_job_name,\n",
    "        StatusEquals='Completed',\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=10\n",
    "    )\n",
    "\n",
    "    latest_training_job_name = training_jobs['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    print(f\"Training jobname: {latest_training_job_name}\")\n",
    "\n",
    "    estimator = PyTorch.attach(latest_training_job_name)\n",
    "\n",
    "    return estimator, latest_training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "**TODO:** This is the part where you can train a model. The type or architecture of the model you use is not important. \n",
    "\n",
    "**Note:** You will need to use the `train.py` script to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** Here you can perform hyperparameter tuning to increase the performance of your model. You are encouraged to \n",
    "- tune as many hyperparameters as you can to get the best performance from your model\n",
    "- explain why you chose to tune those particular hyperparameters and the ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_config['run_hyperparameter_job'] == 1:\n",
    "    best_estimator, tuner = train_hyperparameter_tuning_job(train, test, valid)\n",
    "    tuning_job_name = train_hyp_tuning_job_summarizer(tuner)\n",
    "elif notebook_config['run_hyperparameter_job'] == 0:\n",
    "    best_estimator, tuning_job_name = fetch_hyperparameter_tuning_job(sagemaker_client)\n",
    "else:\n",
    "    print(\"Not a valid selection for run_hyperparameter_job parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = HyperparameterTuningJobAnalytics(\n",
    "    #hyperparameter_tuning_job_name='pytorch-training-240712-1957')\n",
    "    hyperparameter_tuning_job_name=tuning_job_name)\n",
    "\n",
    "jobs = exp.dataframe()\n",
    "\n",
    "jobs.sort_values('FinalObjectiveValue', ascending=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    \"batch-size\": int(best_estimator.hyperparameters()[\"batch-size\"].replace('\"', \"\")),\n",
    "    \"epochs\": best_estimator.hyperparameters()[\"epochs\"],\n",
    "}\n",
    "\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Profiling and Debugging\n",
    "**TODO:** Use model debugging and profiling to better monitor and debug your model training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "\n",
    "debugger_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"100\", \"eval.save_interval\": \"10\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation / Multi Instance\n",
    "**TODO:** We'll use multi-instance traiing to have a faster training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_config['train_a_model'] == 1:\n",
    "    estimator, training_job_name = train_a_model_from_scratch(train, test, valid, \n",
    "                                    role, best_hyperparameters, rules, profiler_config, debugger_config)\n",
    "elif notebook_config['train_a_model'] == 0:\n",
    "    estimator, training_job_name = fetch_a_trained_model()\n",
    "else:\n",
    "    print(\"Not a valid selection for train_a_model parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the debuggin output of our training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader=None,\n",
    "    select_dimensions=[\"CPU\", \"GPU\"],\n",
    "    select_events=[\"total\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory utilizations, we have an expected behavior since the training is going on GPU while we have evaluation phase on CPU. This increases data transfers between CPU and GPU. But because of budget limitation, we prefer to have these transfer instead of having an always on-GPU training and evalution.\n",
    "\n",
    "For CPU and GPU utilizations, we can see some oscilations for both. The utilization oscillations for CPU have higher peeks which can be originated from the computaion load of model evaluaion processes. It also shows that most of times, we are in training instead of evalution; In other words, the training has a bigger portion than evalution in the whole time.\n",
    "\n",
    "Since we have a multi-instance traiing with two instances, all of the paramters we mentioned above shuold be considered for both instances or nodes.\n",
    "\n",
    "Now it's time to take a look at profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the serializers and deserializers\n",
    "jpeg_serializer = IdentitySerializer(\"image/jpeg\")\n",
    "json_serializer = JSONSerializer()\n",
    "json_deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Predictor class\n",
    "class ImagePredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(ImagePredictor, self).__init__(\n",
    "            endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=jpeg_serializer,\n",
    "            deserializer=json_deserializer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the model location using training job name\n",
    "def get_model_location(training_job_name):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    return model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the model location automatically\n",
    "model_location = get_model_location(training_job_name)\n",
    "print(f\"Model location: {model_location}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorchModel object\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_location,\n",
    "    role=role,\n",
    "    entry_point='inference.py',  # Ensure this is the correct path to your script\n",
    "    py_version='py38',  # Ensure this matches the Python version used in training\n",
    "    framework_version='1.12',  # Ensure this matches the PyTorch framework version used in training\n",
    "    predictor_cls=ImagePredictor,\n",
    "    env={\n",
    "        'MODEL_PATH': 'model.pth',\n",
    "        'NUM_CLASSES': '5',\n",
    "        'IMAGE_SIZE': '224'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to an endpoint\n",
    "#predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "\n",
    "# Deploy the model to an endpoint with a larger instance type\n",
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    timeout = 6000\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_presigned_url(bucket_name, object_key, expiration=3600):\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.generate_presigned_url('get_object',\n",
    "                                                    Params={'Bucket': bucket_name,\n",
    "                                                            'Key': object_key},\n",
    "                                                    ExpiresIn=expiration)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    return response\n",
    "\n",
    "bucket_name = 'sagemaker-us-east-1-462848125024'\n",
    "object_key = 'aws_capstone_project/valid_data/3/00037.jpg'\n",
    "\n",
    "presigned_url = generate_presigned_url(bucket_name, object_key)\n",
    "print(f\"Pre-Signed URL: {presigned_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the image URL for inference\n",
    "#request_dict = {\n",
    "#    \"url\": \"https://sagemaker-us-east-1-462848125024.s3.amazonaws.com/aws_capstone_project/valid_data/3/00037.jpg\"\n",
    "#}\n",
    "# Load the image bytes\n",
    "#img_bytes = requests.get(request_dict['url']).content\n",
    "#print(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image bytes using the pre-signed URL\n",
    "img_bytes = requests.get(presigned_url).content\n",
    "img = Image.open(BytesIO(img_bytes)).convert('RGB')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the request dictionary\n",
    "request_dict = {\n",
    "    \"url\": presigned_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "response = predictor.predict(img_bytes, initial_args={\"ContentType\": \"image/jpeg\"})\n",
    "#response = predictor.predict(json.dumps(request_dict), initial_args={\"ContentType\": \"application/json\"})\n",
    "# Process the prediction response\n",
    "print(\"Prediction response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (delete) the endpoint when done to avoid unnecessary costs\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
